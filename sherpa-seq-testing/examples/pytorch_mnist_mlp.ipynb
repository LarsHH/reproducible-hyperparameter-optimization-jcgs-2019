{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SHERPA is a Python library for hyperparameter tuning of machine learning models.\n",
    "Copyright (C) 2018  Lars Hertel, Peter Sadowski, and Julian Collado.\n",
    "\n",
    "This file is part of SHERPA.\n",
    "\n",
    "SHERPA is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "SHERPA is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with SHERPA.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import sherpa\n",
    "from sherpa.algorithms import Genetic\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_classes = 10\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [sherpa.Discrete('hidden_size', [16, 512]),\n",
    "              sherpa.Discrete('n_layers', [1, 10]),\n",
    "              sherpa.Choice('activation', [F.relu, F.tanh, F.sigmoid]),\n",
    "              sherpa.Continuous('lr',[1e-4,1e-2]),\n",
    "              sherpa.Continuous('dropout',[0.0,1.0])]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algorithm= Genetic(max_num_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,in_size,out_size,n_layers,hidden_size,act,dropout):\n",
    "        super(MLP,self).__init__()\n",
    "        self.n_layers=n_layers\n",
    "        self.act=act\n",
    "        for i in range(n_layers):\n",
    "            if i==0:\n",
    "                layer_in_size=in_size\n",
    "            else:\n",
    "                layer_in_size=hidden_size\n",
    "            if i==(n_layers-1):\n",
    "                layer_out_size=out_size\n",
    "            else:\n",
    "                layer_out_size=hidden_size\n",
    "            \n",
    "            setattr(self,'dense_{}'.format(i),nn.Linear(layer_in_size,layer_out_size))\n",
    "            \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=x\n",
    "        for i in range(self.n_layers):\n",
    "            if i==(self.n_layers-1):\n",
    "                out=getattr(self,'dense_{}'.format(i))(self.dropout(out))\n",
    "            else:\n",
    "                out=self.act(getattr(self,'dense_{}'.format(i))(self.dropout(out)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train).type(torch.long)),batch_size=batch_size,drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "x_test_tensor=torch.from_numpy(x_test)\n",
    "y_test_tensor=torch.from_numpy(y_test).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=algorithm,\n",
    "                     lower_is_better=False)\n",
    "for trial in study:\n",
    "    print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
    "    mlp=MLP(x_train.shape[1],num_classes,\n",
    "            trial.parameters['n_layers'],\n",
    "            trial.parameters['hidden_size'],\n",
    "            trial.parameters['activation'],\n",
    "            trial.parameters['dropout'])\n",
    "    mlp.train()\n",
    "    optimizer=optim.Adam(mlp.parameters(), lr=trial.parameters['lr'])\n",
    "    for i in range(epochs):\n",
    "        for x_batch, y_batch in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            out=mlp(x_batch)\n",
    "            loss=criterion(out,y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mlp.eval()        \n",
    "        val_acc=(mlp(x_test_tensor).argmax(dim=1)==y_test_tensor).type(torch.float32).mean().item()\n",
    "        print('val_acc: {}'.format(val_acc))\n",
    "        study.add_observation(trial=trial,\n",
    "                          iteration=epochs,\n",
    "                          objective=val_acc)\n",
    "        if study.should_trial_stop(trial):\n",
    "            break \n",
    "    study.finalize(trial=trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(study.get_best_result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
